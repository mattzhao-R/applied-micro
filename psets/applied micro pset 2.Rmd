---
title: "Applied Micro PSET 2"
author: "Matthew Zhao"
date: "2023-01-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(stargazer)
library(xtable)
options(scipen=999)
```

## Question 1

```{r}
df <- read_stata('data/gpa2.DTA')
```

### a)

\begin{equation}
\tag{1}
colgpa_i = \beta_0 + \beta_1 female_i + U_i
\end{equation}

```{r, warning=FALSE}
model_a <- lm(colgpa ~ female, data = df)
stargazer(model_a,type='text',digits=3,
          title='Table 1 - OLS Estimates of (1)')
```

From Table 1, we see that the OLS estimate for $\beta_1$ in (1) is $\hat{\beta}_1 = 0.142$ and is significant at the 1% significance level. We interpret this coefficient as an individual who is female has a college gpa that is on average approximately 0.142 higher than someone who is not. We can interpret the OLS estimate of $\beta_0$ which is $\hat{\beta}_0 = 2.589$ as the approximate average college gpa of someone who is not female. 

### b)

```{r, warning=FALSE}
temp <- df %>%
  mutate(res_b = lm(colgpa ~ female, data = df)$residuals)

ggplot(data = temp, aes(x = res_b)) +
  geom_histogram(binwidth = 0.2) +
  facet_wrap(~female,nrow=2) + 
  labs(x = 'Residuals', y = 'Count')
```

Homoskedasticity is defined as $Var(U_i|X_i) = \sigma^2$ for all $i=1,..,n$. From the distributions of residuals above, we see that the spread in residuals is slightly smaller for observations with female = 1 i.e. individuals who are female than for those who are not. This could potentially be evidence of heteroskedasticity. 

```{r}
ggplot(data = df, aes(x = colgpa)) +
  geom_histogram(binwidth = 0.2) +
  facet_wrap(~female,nrow=2) + 
  labs(x = 'College GPA', y = 'Count')
```

The distribution of residuals conditional on female is a reflection of the distribution of colgpa conditional on female because in this regression the only explanatory variable we are using is female. As a result, the spread of college gpa conditional on female directly determines the spread of residuals conditional on female e.g. individuals who are not female tend to have a wider range of gpas means that the residuals for individuals who are not female will be more spread out since they will be further from the fitted values from the regression and hence have a wider range of residuals. 

### c)

\begin{equation}
\tag{2}
colgpa_i = \beta_0 + \beta_1 female_i + \beta_2 sat_i + V_i
\end{equation}

```{r, warning=FALSE}
model_c <- lm(colgpa ~ female + sat, data = df)
stargazer(model_c,type='text',digits=3,
          title='Table 2 - OLS Estimates of (2)')
```

From Table 2, we see that the OLS estimate for $\beta_1$ in (2) is $\hat{\beta}_1 = 0.231$ and that $\beta_2$ in (2) is $\hat{\beta}_2 = 0.002$, with both significant at the 1% significance level. We interpret the first coefficient as an individual who is female has a college gpa that is on average approximately 0.231 higher than someone who is not with the same sat score. We interpret the second coefficient as an individual would have a college gpa that is approximately 0.002 higher on average if they had an sat score than is 1 point higher and are the same gender. The OLS estimate of $\beta_0$ which is $\hat{\beta}_0 = 2.589$ represents the approximate average college gpa of someone who is not female and has an sat score of 0. However, since the sample does not contain anyone with an sat score of 0, this is an extrapolation and hence may not be an accurate interpretation. 

Since the partial effect of female increased after controlling for sat score, the change suggests that female is negatively correlated with sat score. 

In comparing models (1) and (2), we are looking at potential OVB of SLR vs MLR, where we can decompose the OLS estimate of $\beta_1$ from the SLR (from here on referred to as $\tilde{\beta}_1$) as $\tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_2 \tilde{\delta}_1$ where $\hat{\beta}_1$ and $\hat{\beta}_2$ are the OLS estimates of the parameters of (2) and $ \tilde{\delta}_1$ is from the OLS estimate of $sat = \delta_0 + \delta_1 female + U$. Specifically, we can derive the implied OVB of leaving out sat using the first equation from earlier, where $E[\tilde{\beta}_1] - \hat{\beta}_1 = \hat{\beta}_2 \tilde{\delta}_1$. Using the values from part (b) and below, we find that the implied OVB of leaving out sat in the regression in (a) is around $0.002 * -43.07331 = -0.086$, which is roughly the difference between $\hat{\beta}_1$ and $\tilde{\beta}_1$ ($\tilde{\beta}_1 - \hat{\beta}_1 = 0.142 - 0.231 = -0.089)$. 

```{r}
temp <- lm(sat ~ female, data = df)
temp$coefficients
```

### d)

While the OLS estimator $\hat{\beta}$ of the parameter vector $\beta$ in (2) remains unbiased and consistent, the variance of this estimator $Var(\hat{\beta})$ is now biased.

### e)

**Breuch-Pagan**

```{r, warning=FALSE}
U_hat <- model_c$residuals
U_reg <- lm(U_hat^2 ~ df$female + df$sat)
U_rsq <- summary(U_reg)$r.squared
n <- nrow(df)
k <- 2

F_stat <- (U_rsq/k) / ((1-U_rsq)/(n-k-1))
pf(F_stat,k,n-k-1,lower.tail = F)
```

**White Test**

```{r, warning=FALSE}
white_reg <- lm(U_hat^2 ~ df$colgpa + I(df$colgpa)^2)

# linearHypothesis(white_reg, c("colgpa=0", "expenditure=0"))
# stargazer(white_reg,type='text',digits=3,
#           title='Table 4 - White Test for Heteroskedasticity')
```

We do find some evidence for heteroskedasticity since we see that the BP test rejects the null of homoskedasticity at the 1% significance level. However the F-stat for the White Test is large and significant, meaning that the coefficients are likely to be non-zero and the test fails to reject. **ask about this**


### f)

```{r, warning=FALSE, message=FALSE}
library(lmtest)
library(sandwich)

summary(model_c)
```

```{r}
coeftest(model_c, vcov = vcovHC(model_c, type = 'HC1'))
```

The standard errors are lower while the t-statistics are larger. We expect this since we saw earlier that the variance in residuals is lower for females (female = 1) than for males (female = 0). This means that the residuals are negatively correlated with $\sigma_i^2$. As a result, since the formula for robust standard errors is $\hat{\sigma}(\hat{\beta_j}) = \displaystyle \sqrt{\frac{\sum_{i=1}^n \hat{\epsilon}_{ij}^2 \hat{U}^2_i}{SSR_j^2}}$ and for non robust is $\hat{\sigma}(\hat{\beta_j}) = \displaystyle \frac{\hat{\sigma}_U}{\sqrt{SST_j(1-R^2_j)}}$, we see that the robust standard errors are smaller than nonrobust when the variance and residuals are negatively correlated as we see here. 

### g)

```{r, warning=FALSE}
m1 <- lm(colgpa ~ female + sat, data = df)
m2 <- lm(colgpa ~ female + sat + I(sat^2), data = df)
m3 <- lm(colgpa ~ female + sat + I(sat^2) + I(sat^3), data = df)
models <- list(m1,m2,m3)

stargazer(models,type='text',digits=3,
          title='Table 5 - Different Functional Forms Variants of (2)')
```

We find that adding polynomial terms for sat works up to the second power, with the coefficient being statistically significant at the 1% level. Subsequent terms add little to the regression and are not significant.

Non-random selection of students into college may explain this weakly negative relationship between college gpa and sat score since generally, students go to the best school they are able to. As a result, they typically will be taking classes with students who are at similar levels of intelligence/work ethic. With classes being curved, there is a limit to how well you can do based on how well other students in your class do. This has the largest effect on students at the higher end of the sat distribution, since the sat is an absolute measure of high school performance, while college gpa is, to an extent, a relative measure of performance in college. Hence, the quadratic form best fits the functional form relationship between college gpa and sat.  

### h)

*corr w sat, reduce coef but not variance, multicollinear/unique variation in sat leads to coef but incl hsperc reduces this*
```{r, warning=FALSE}
with_hsperc <- lm(colgpa ~ female + sat + hsperc, data = df)
hsperc_sat <- lm(sat ~ female + hsperc, data = df)
stargazer(list(model_c,with_hsperc, hsperc_sat),type='text',digits=4,
          title='Table 6 - Comparison of Model with hsperc')
```

hsperc represents the class percentile a student was in in high school starting from the top. Omitting hsperc causes $\hat{\beta}_2$ to be larger because a student's class percentile is most likely correlated with sat score. As a result of this relationship, including hsperc would reduce the unique variation in sat and cause this coefficient to fall. Additionally, this has minimal impact on the variance of $\hat{\beta}_2$ since 

### i)



### j)



### k)



## Problem 2

### a)


### b)


## Problem 3






