---
title: "Applied Micro PSET 2"
author: "Matthew Zhao working with Cedric Elkouh"
date: "2023-01-16"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(stargazer)
library(xtable)
options(scipen=999)
```

## Question 1

```{r}
df <- read_stata('data/gpa2.DTA')
```

### a)

\begin{equation}
\tag{1}
colgpa_i = \beta_0 + \beta_1 female_i + U_i
\end{equation}

```{r, warning=FALSE}
model_a <- lm(colgpa ~ female, data = df)
stargazer(model_a,type='text',digits=3,
          title='Table 1 - OLS Estimates of (1)')
```

From Table 1, we see that the OLS estimate for $\beta_1$ in (1) is $\hat{\beta}_1 = 0.142$ and is significant at the 1% significance level. We interpret this coefficient as an individual who is female has a college gpa that is on average approximately 0.142 higher than someone who is not. We can interpret the OLS estimate of $\beta_0$ which is $\hat{\beta}_0 = 2.589$ as the approximate average college gpa of someone who is not female. 

### b)

```{r, warning=FALSE}
temp <- df %>%
  mutate(res_b = lm(colgpa ~ female, data = df)$residuals)

ggplot(data = temp, aes(x = res_b)) +
  geom_histogram(binwidth = 0.2) +
  facet_wrap(~female,nrow=2) + 
  labs(x = 'Residuals', y = 'Count')
```

Homoskedasticity is defined as $Var(U_i|X_i) = \sigma^2$ for all $i=1,..,n$. From the distributions of residuals above, we see that the spread in residuals is slightly smaller for observations with female = 1 i.e. individuals who are female than for those who are not. This could potentially be evidence of heteroskedasticity. 

```{r}
ggplot(data = df, aes(x = colgpa)) +
  geom_histogram(binwidth = 0.2) +
  facet_wrap(~female,nrow=2) + 
  labs(x = 'College GPA', y = 'Count')
```

The distribution of residuals conditional on female is a reflection of the distribution of colgpa conditional on female because in this regression the only explanatory variable we are using is female. As a result, the spread of college gpa conditional on female directly determines the spread of residuals conditional on female e.g. individuals who are not female tend to have a wider range of gpas means that the residuals for individuals who are not female will be more spread out since they will be further from the fitted values from the regression and hence have a wider range of residuals. 

### c)

\begin{equation}
\tag{2}
colgpa_i = \beta_0 + \beta_1 female_i + \beta_2 sat_i + V_i
\end{equation}

```{r, warning=FALSE}
model_c <- lm(colgpa ~ female + sat, data = df)
stargazer(model_c,type='text',digits=3,
          title='Table 2 - OLS Estimates of (2)')
```

From Table 2, we see that the OLS estimate for $\beta_1$ in (2) is $\hat{\beta}_1 = 0.231$ and that $\beta_2$ in (2) is $\hat{\beta}_2 = 0.002$, with both significant at the 1% significance level. We interpret the first coefficient as an individual who is female has a college gpa that is on average approximately 0.231 higher than someone who is not with the same sat score. We interpret the second coefficient as an individual would have a college gpa that is approximately 0.002 higher on average if they had an sat score than is 1 point higher and are the same gender. The OLS estimate of $\beta_0$ which is $\hat{\beta}_0 = 2.589$ represents the approximate average college gpa of someone who is not female and has an sat score of 0. However, since the sample does not contain anyone with an sat score of 0, this is an extrapolation and hence may not be an accurate interpretation. 

Since the partial effect of female increased after controlling for sat score, the change suggests that female is negatively correlated with sat score. 

In comparing models (1) and (2), we are looking at potential OVB of SLR vs MLR, where we can decompose the OLS estimate of $\beta_1$ from the SLR (from here on referred to as $\tilde{\beta}_1$) as $\tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_2 \tilde{\delta}_1$ where $\hat{\beta}_1$ and $\hat{\beta}_2$ are the OLS estimates of the parameters of (2) and $\tilde{\delta}_1$ is from the OLS estimate of $sat = \delta_0 + \delta_1 female + U$. Specifically, we can derive the implied OVB of leaving out sat using the first equation from earlier, where $E[\tilde{\beta}_1] - \hat{\beta}_1 = \hat{\beta}_2 \tilde{\delta}_1$. Using the values from part (b) and below, we find that the implied OVB of leaving out sat in the regression in (a) is around $0.002 * -43.07331 = -0.086$, which is roughly the difference between $\hat{\beta}_1$ and $\tilde{\beta}_1$ ($\tilde{\beta}_1 - \hat{\beta}_1 = 0.142 - 0.231 = -0.089$). 

```{r}
temp <- lm(sat ~ female, data = df)
temp$coefficients
```

### d)

While the OLS estimator $\hat{\beta}$ of the parameter vector $\beta$ in (2) remains unbiased and consistent, the variance of this estimator $Var(\hat{\beta})$ is now biased.

### e)

**Breuch-Pagan**

```{r, warning=FALSE}
U_hat <- model_c$residuals
U_reg <- lm(U_hat^2 ~ df$female + df$sat)
U_rsq <- summary(U_reg)$r.squared
n <- nrow(df)
k <- 2

F_stat <- (U_rsq/k) / ((1-U_rsq)/(n-k-1))
pf(F_stat,k,n-k-1,lower.tail = F)
```

**White Test**

```{r, warning=FALSE}
white_reg <- lm(U_hat^2 ~ df$colgpa + I(df$colgpa)^2)

stargazer(white_reg,type='text',digits=3,
          title='Table 4 - White Test for Heteroskedasticity')
```

We do find some evidence for heteroskedasticity since we see that the BP test rejects the null of homoskedasticity at the 1% significance level. However the F-stat for the White Test is large and significant, meaning that the coefficients are likely to be non-zero and the test fails to reject. 


### f)

```{r, warning=FALSE, message=FALSE}
library(lmtest)
library(sandwich)

summary(model_c)
```

```{r}
coeftest(model_c, vcov = vcovHC(model_c, type = 'HC1'))
```

The standard errors are lower while the t-statistics are larger. We expect this since we saw earlier that the variance in residuals is lower for females (female = 1) than for males (female = 0). This means that the residuals are negatively correlated with $\sigma_i^2$. As a result, since the formula for robust standard errors is $\hat{\sigma}(\hat{\beta_j}) = \displaystyle \sqrt{\frac{\sum_{i=1}^n \hat{\epsilon}_{ij}^2 \hat{U}^2_i}{SSR_j^2}}$ and for non robust is $\hat{\sigma}(\hat{\beta_j}) = \displaystyle \frac{\hat{\sigma}_U}{\sqrt{SST_j(1-R^2_j)}}$, we see that the robust standard errors are smaller than nonrobust when the variance and residuals are negatively correlated as we see here. 

### g)

```{r, warning=FALSE}
m1 <- lm(colgpa ~ female + sat, data = df)
m2 <- lm(colgpa ~ female + sat + I(sat^2), data = df)
m3 <- lm(colgpa ~ female + sat + I(sat^2) + I(sat^3), data = df)
models <- list(m1,m2,m3)

stargazer(models,type='text',digits=3,
          title='Table 5 - Different Functional Forms Variants of (2)')
```

We find that adding polynomial terms for sat works up to the second power, with the coefficient being statistically significant at the 1% level. Subsequent terms add little to the regression and are not significant.

Non-random selection of students into college may explain this weakly negative relationship between college gpa and sat score since generally, students go to the best school they are able to. As a result, they typically will be taking classes with students who are at similar levels of intelligence/work ethic. With classes being curved, there is a limit to how well you can do based on how well other students in your class do. This has the largest effect on students at the higher end of the sat distribution, since the sat is an absolute measure of high school performance, while college gpa is, to an extent, a relative measure of performance in college. Hence, the quadratic form best fits the functional form relationship between college gpa and sat.  

### h)

```{r, warning=FALSE}
with_hsperc <- lm(colgpa ~ female + sat + hsperc, data = df)
stargazer(list(with_hsperc,model_c),type='text',digits=4,
          title='Table 6 - Comparison of Model with hsperc')
```

```{r, warning=FALSE}
sat_nohsperc <- lm(sat ~ female, data = df)
sat_whsperc <- lm(sat ~ female + hsperc, data = df)

stargazer(list(sat_whsperc,sat_nohsperc),type='text',digits=4,
          title='Table 7 - sat on explanatory vars')
```

hsperc represents the class percentile a student was in in high school starting from the top. Omitting hsperc causes $\hat{\beta}_2$ to be larger because a student's class percentile is most likely correlated with sat score. As a result of this relationship, including hsperc would reduce the unique variation in sat and cause this coefficient to fall. Additionally, omitting hsperc should reduce the variance of $\hat{\beta}_2$ since $\hat{\sigma}(\hat{\beta_j}) = \displaystyle \frac{\hat{\sigma}_U}{\sqrt{SST_j(1-R^2_j)}}$. Omitting hsperc would reduce $R^2_j$ since the other explanatory variables account for less of the variation in sat. As a result, because the denominator is larger, the overall variance/se is smaller. 

### i)

The idea randomized experiment would be to take a large group of individuals right before they enter college and clone a female version of them with the same background, experiences, etc or turn them female  so that we can completely isolate the causal effect of gender on college gpa. 

### j)

Besides the practical difficulties, empirically, it is difficult to implement the ideal randomized experiment we described in (i) because an individual's life is largely shaped by their assigned sex at birth, meaning that it is very unlikely for MLR.4 to hold since there are innumerable unobservables that affect an individual. An example in this case would be that even if we could change some individual's genders, their past experience in their initial gender could have an impact on their college gpa. More concretely, there are numerous papers which show differential education and educational opportunities between men and women starting from a young age. As a result, switching from male to female is simply not comparable to being female your whole life, making it empirically challenging to learn the true effect of being female on college gpa. 

Conceptually, it does not make sense to estimate a causal effect of female. Firstly, this is because, while quantifying the gender gap in educational achievement is of interest, practically it is difficult to define what being female means. Would we define being female as biological in terms of horomones and physical traits, or focus on identity and perceived gender? These questions make it conceptually difficult to understand what a causal effect of female would even mean, hence the question not making sense.

### k)

I do not find MLR.4 credible in the population model. This is because it is very likely that any number of unobservables could be correlated with female and affect college gpa e.g. high school performance, work ethic, geography, cultural values, number/gender/order of siblings, etc. I would want to collect information on the already mentioned variables in addition to family wealth, parental education, schools attended/attending, etc. 

I believe that including the most important omitted variable in (1) (work ethic) would reduce $\hat{\beta}_1$ since the estimator is initially positive in the OLS estimate of (1) and work ethic is likely to be positively correlated with being female. Additionally, since work ethic is positively correlated with being female, it is likely to increase the variance of $\hat{\beta}_1$ since it reduces the unique variance of female in explaining college gpa. 

## Problem 2

### a)

Given $X_2$ is independent of $X_3$ and that $X_1$ is correlated with $X_2$ and $X_3$:

$X_2 = \delta_0^2 + \delta_1^2 X_1 + \epsilon_2 \Rightarrow \epsilon_2 = X_2 - \delta_0^2 - \delta_1^2 X_1$ and $X_3 = \delta_0^3 + \delta_1^3 X_1 + \epsilon_3 \Rightarrow \epsilon_3 = X_3 - \delta_0^3 - \delta_1^3 X_1$. 

$Cov(\epsilon_2,\epsilon_3) = Cov(X_2 - \delta_0^2 - \delta_1^2 X_1, X_3 - \delta_0^3 - \delta_1^3 X_1)$
$= -\delta_1^3Cov(X_2, X_1) - \delta_1^2Cov(X_1, X_3) + \delta_1^2 \delta_1^3 Var(X_1)$

Since we know that $X_1$ is correlated with $X_2$ and $X_3$ and variance is non-negative, $Cov(\epsilon_2,\epsilon_3) \ne 0$ and so they are correlated. 

From there it is sufficient to show that if $\alpha_2$ in the equation $X_3 = \alpha_0 +\alpha_1 X_1 + \alpha_2 X_2 + U$ is nonzero, the OLS estimate for $\beta_2$ in the equation $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$ is biased. Since $\gamma_2 = \alpha_2$ where $\gamma_2$ is from the equation $\epsilon_3 = \gamma_0 + \gamma_2 \epsilon_2 + V$, and we previously showed that $\rho_{\epsilon_2,\epsilon_3} \ne 0$, $\gamma_2=\alpha_2 \ne 0$. Hence, the OLS estimate for $\beta_2$ will be biased. 

### b)

Since $X_2$ is now independent of $X_1$, it follows from part (a) that $\hat{\beta}_2$ is now unbiased.

For $\hat{\beta}_1$:

From 3.23: $E[\hat{\beta}_1] = \beta_1 + \beta_3 \tilde{\delta}_1 \Rightarrow OVB = E[\hat{\beta}_1] - \beta_1 = \beta_3 \tilde{\delta}_1$

$\tilde{\delta}_1 = \displaystyle \frac{Cov(X_1, X_3)}{Var(X_1)} = \frac{\frac{1}{n} \sum_{i=1}^n (X_{1i} - \bar{X}_1)(X_{3i} - \bar{X}_3)}{\frac{1}{n}\sum_{i=1}^n (X_{1i} - \bar{X}_1)^2}$

$\sum (X_{1i} - \bar{X}_1)(X_{3i} - \bar{X}_3) = \sum X_{1i}X_{3i} - \sum X_{1i}\bar{X}_3 - \sum \bar{X}_1 X_{3i} + \sum \bar{X}_1 \bar{X}_3$

$= \sum X_{1i}X_{3i} - \bar{X}_1 X_{3i} + \sum \bar{X}_1 \bar{X}_3 - X_{1i}\bar{X}_3$

$= \sum (X_{1i} - \bar{X}_1) X_{3i} + n\bar{X}_3\sum \bar{X}_1  - X_{1i}$

Since $\sum \bar{X}_1  - X_{1i} = 0$, the prior expression is equal to $\sum (X_{1i} - \bar{X}_1) X_{3i}$

Therefore, our approximate OVB for $\beta_1$ is $\displaystyle \frac{\sum_{i=1}^n (X_{1i} - \bar{X}_1) X_{3i}}{\sum_{i=1}^n (X_{1i} - \bar{X}_1)^2}$


## Problem 3

To preface my answer to this section, I have decided to change my paper to replicate from my answer in the previous week since I was not able to work with the data for that paper. As a result, I have now chosen a different paper for which I have the data files (linked [**here**](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/RQOCKS))

### a)

The main dependent variable of interest is stock portfolio allocation. The main explanatory variables are retirement, marital status, family labor income, net worth, pension income, number of children, age, health care expenditures. Since this is panel data, the author also includes household and time fixed effects.

### b)

I have been able to replicate these in stata but I aim to translate these into R for my final project. 

